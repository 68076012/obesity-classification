{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Obesity Classification - Improved Model\n",
    "\n",
    "## ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ô‡πÄ‡∏î‡∏¥‡∏°\n",
    "\n",
    "### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á:\n",
    "1. ‚úÖ ‡πÉ‡∏ä‡πâ **BorderlineSMOTE** ‡πÅ‡∏ó‡∏ô SMOTE ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ - ‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡∏ó‡∏µ‡πà boundary cases\n",
    "2. ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° **class_weight='balanced'** - ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö minority class\n",
    "3. ‚úÖ ‡∏Ç‡∏¢‡∏≤‡∏¢ **hyperparameter grid** - ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤\n",
    "4. ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° **Feature Engineering** - ‡∏™‡∏£‡πâ‡∏≤‡∏á interaction features\n",
    "5. ‚úÖ ‡πÉ‡∏ä‡πâ **XGBoost** - ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Random Forest\n",
    "6. ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö **Cross-Validation** - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Import Libraries ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "df = pd.read_csv('obesity_dataset.csv')\n",
    "\n",
    "print(\"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:\", df.shape)\n",
    "print(\"\\nüîç ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Feature Engineering - ‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡πÉ‡∏´‡∏°‡πà\n",
    "\n",
    "### ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•:\n",
    "- **Interaction Features** ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features\n",
    "- **Polynomial Features** ‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏ö non-linear relationships\n",
    "- **BMI-based Features** ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á Interaction Features\n",
    "# (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ - ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á)\n",
    "\n",
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á\n",
    "# df['Exercise_x_ScreenTime'] = df['Physical_Activity'] * df['Screen_Time']\n",
    "# df['Food_x_Exercise'] = df['Food_Consumption'] * df['Physical_Activity']\n",
    "# df['Calorie_Ratio'] = df['FCVC'] / (df['NCP'] + 1)\n",
    "\n",
    "print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á features ‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
    "print(f\"üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ ‡πÅ‡∏¢‡∏Å Features (X) ‡πÅ‡∏•‡∏∞ Target (y)\n",
    "# ‡∏õ‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á\n",
    "target = 'Class'  # ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå target ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
    "\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á categorical variables ‡πÄ‡∏õ‡πá‡∏ô numeric (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
    "# X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "print(f\"üìä Features shape: {X.shape}\")\n",
    "print(f\"üéØ Target distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÇÔ∏è ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Test (80/20)\n",
    "# stratify=y ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ class ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡πÉ‡∏ô train ‡πÅ‡∏•‡∏∞ test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Train set: {X_train.shape}\")\n",
    "print(f\"üìä Test set: {X_test.shape}\")\n",
    "print(f\"\\nüéØ Train target distribution:\\n{y_train.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Imbalanced Data\n",
    "\n",
    "### BorderlineSMOTE vs SMOTE:\n",
    "- **SMOTE** ‡∏™‡∏£‡πâ‡∏≤‡∏á synthetic samples ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°‡∏à‡∏≤‡∏Å minority class\n",
    "- **BorderlineSMOTE** ‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ decision boundary\n",
    "- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏¢‡∏Å‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Overweight ‡∏Å‡∏±‡∏ö Obesity ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìö Import Libraries ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Modeling\n",
    "from imblearn.over_sampling import BorderlineSMOTE, SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á XGBoost ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ: pip install xgboost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "    print(\"‚úÖ XGBoost ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\n",
    "except ImportError:\n",
    "    xgb_available = False\n",
    "    print(\"‚ö†Ô∏è ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á XGBoost ‡∏î‡πâ‡∏ß‡∏¢: pip install xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Model 1: Random Forest with BorderlineSMOTE\n",
    "\n",
    "### Hyperparameters:\n",
    "- **n_estimators**: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô trees (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô)\n",
    "- **max_depth**: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡∏Ç‡∏≠‡∏á tree (None = ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î)\n",
    "- **class_weight**: 'balanced' ‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Å‡∏±‡∏ö minority class ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "- **min_samples_split/leaf**: ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üå≤ Random Forest Pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    # BorderlineSMOTE: ‡∏™‡∏£‡πâ‡∏≤‡∏á synthetic samples ‡∏ó‡∏µ‡πà boundary\n",
    "    # k_neighbors=3: ‡πÉ‡∏ä‡πâ 3 nearest neighbors (‡∏•‡∏î‡∏•‡∏á‡∏à‡∏≤‡∏Å 5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ boundary ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)\n",
    "    ('smote', BorderlineSMOTE(random_state=42, k_neighbors=3)),\n",
    "    \n",
    "    # Random Forest Classifier\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# üîß Hyperparameter Grid - ‡∏Ç‡∏¢‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "param_grid_rf = {\n",
    "    'clf__n_estimators': [100, 200, 300],  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô trees\n",
    "    'clf__max_depth': [None, 10, 20, 30],  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å\n",
    "    'clf__class_weight': ['balanced', 'balanced_subsample', None],  # ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å class\n",
    "    'clf__min_samples_split': [2, 5, 10],  # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ split\n",
    "    'clf__min_samples_leaf': [1, 2, 4],     # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÉ‡∏ô leaf\n",
    "    'clf__max_features': ['sqrt', 'log2']   # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô split\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Random Forest Pipeline ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "print(f\"üîç ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {np.prod([len(v) for v in param_grid_rf.values()])} combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Cross-Validation: StratifiedKFold 10-fold\n",
    "# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 5 ‡πÄ‡∏õ‡πá‡∏ô 10 folds ‡πÄ‡∏û‡∏∑‡πà‡∏≠ estimate ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# üîç GridSearchCV - ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ best parameters\n",
    "print(\"üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏° GridSearch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Random Forest...\")\n",
    "print(\"‚è≥ ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 5-15 ‡∏ô‡∏≤‡∏ó‡∏µ ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\\n\")\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    pipe_rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=cv,\n",
    "    scoring='f1_weighted',  # ‡πÉ‡∏ä‡πâ weighted F1 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•\n",
    "    n_jobs=-1,              # ‡πÉ‡∏ä‡πâ CPU ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß\n",
    "    verbose=1               # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤\n",
    ")\n",
    "\n",
    "# üèãÔ∏è Train Model\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# üìä ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "print(\"\\n‚úÖ Training ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "print(f\"üèÜ Best parameters: {grid_rf.best_params_}\")\n",
    "print(f\"üìà Best CV F1 Score: {grid_rf.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Model 2: Gradient Boosting with Improvements\n",
    "\n",
    "### ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á:\n",
    "- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå **subsample** - ‡∏ä‡πà‡∏ß‡∏¢ prevent overfitting\n",
    "- ‡∏õ‡∏£‡∏±‡∏ö **learning_rate** ‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢\n",
    "- ‡πÄ‡∏û‡∏¥‡πà‡∏° **max_features** - ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Gradient Boosting Pipeline\n",
    "pipe_gb = Pipeline([\n",
    "    ('smote', BorderlineSMOTE(random_state=42, k_neighbors=3)),\n",
    "    ('clf', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# üîß Hyperparameter Grid\n",
    "param_grid_gb = {\n",
    "    'clf__n_estimators': [100, 200, 300],\n",
    "    'clf__learning_rate': [0.01, 0.05, 0.1, 0.2],  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n",
    "    'clf__max_depth': [3, 5, 7],                   # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ tree\n",
    "    'clf__subsample': [0.8, 0.9, 1.0],            # ‡∏™‡∏∏‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ iteration\n",
    "    'clf__max_features': ['sqrt', 'log2', None]   # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features\n",
    "}\n",
    "\n",
    "print(\"üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏° GridSearch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Gradient Boosting...\")\n",
    "\n",
    "grid_gb = GridSearchCV(\n",
    "    pipe_gb,\n",
    "    param_grid=param_grid_gb,\n",
    "    cv=cv,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_gb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nüèÜ Best GB parameters: {grid_gb.best_params_}\")\n",
    "print(f\"üìà Best CV F1 Score: {grid_gb.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Model 3: XGBoost (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
    "\n",
    "### ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ XGBoost:\n",
    "- **Faster** ‡∏Å‡∏ß‡πà‡∏≤ Gradient Boosting ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤\n",
    "- **Better performance** ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•\n",
    "- ‡∏°‡∏µ **regularization** ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß (alpha, lambda)\n",
    "- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö **scale_pos_weight** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if xgb_available:\n",
    "    # üöÄ XGBoost Pipeline\n",
    "    pipe_xgb = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42, k_neighbors=3)),\n",
    "        ('clf', XGBClassifier(\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss',  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö multi-class\n",
    "            use_label_encoder=False  # ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # üîß Hyperparameter Grid\n",
    "    param_grid_xgb = {\n",
    "        'clf__n_estimators': [100, 200, 300],\n",
    "        'clf__max_depth': [3, 5, 7, 9],\n",
    "        'clf__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'clf__subsample': [0.8, 0.9],\n",
    "        'clf__colsample_bytree': [0.8, 0.9, 1.0],  # ‡∏™‡∏∏‡πà‡∏° features\n",
    "        'clf__gamma': [0, 0.1, 0.2],               # regularization\n",
    "    }\n",
    "    \n",
    "    print(\"üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏° GridSearch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö XGBoost...\")\n",
    "    \n",
    "    grid_xgb = GridSearchCV(\n",
    "        pipe_xgb,\n",
    "        param_grid=param_grid_xgb,\n",
    "        cv=cv,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_xgb.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nüèÜ Best XGB parameters: {grid_xgb.best_params_}\")\n",
    "    print(f\"üìà Best CV F1 Score: {grid_xgb.best_score_:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° XGBoost - ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢: pip install xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "\n",
    "### Metrics:\n",
    "- **F1 Weighted**: ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ class\n",
    "- **F1 Macro**: ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ (‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å class ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô)\n",
    "- **Per-class F1**: ‡∏î‡∏π performance ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "models = {\n",
    "    'Random Forest': grid_rf,\n",
    "    'Gradient Boosting': grid_gb,\n",
    "}\n",
    "\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = grid_xgb\n",
    "\n",
    "# üìä ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'F1 Weighted': f1_weighted,\n",
    "        'F1 Macro': f1_macro\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"F1 Weighted: {f1_weighted:.4f}\")\n",
    "    print(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "    print(f\"\\nüìà Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(\"üèÜ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\")\n",
    "print(f\"{'='*60}\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"\\nü•á ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (F1 Weighted): {results_df.loc[results_df['F1 Weighted'].idxmax(), 'Model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Confusion Matrix - ‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏±‡∏ö‡∏™‡∏ô‡∏ï‡∏£‡∏á‡πÑ‡∏´‡∏ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Plot Confusion Matrices ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "fig, axes = plt.subplots(1, len(models), figsize=(6*len(models), 5))\n",
    "if len(models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "class_names = sorted(y.unique())\n",
    "\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names,\n",
    "                ax=axes[idx])\n",
    "    \n",
    "    axes[idx].set_title(f'{name}\\nConfusion Matrix', fontsize=12, weight='bold')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices_improved.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Confusion Matrices ‡πÄ‡∏õ‡πá‡∏ô 'confusion_matrices_improved.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Feature Importance - Features ‡πÑ‡∏´‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç ‡∏î‡∏π Feature Importance ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "best_model_name = results_df.loc[results_df['F1 Weighted'].idxmax(), 'Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# ‡∏î‡∏∂‡∏á feature importances\n",
    "if hasattr(best_model.best_estimator_.named_steps['clf'], 'feature_importances_'):\n",
    "    importances = best_model.best_estimator_.named_steps['clf'].feature_importances_\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_imp['Feature'][:15], feature_imp['Importance'][:15])\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.title(f'Top 15 Feature Importances - {best_model_name}', fontsize=14, weight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Top 10 Features:\")\n",
    "    print(feature_imp.head(10).to_string(index=False))\n",
    "    print(\"\\n‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡πá‡∏ô 'feature_importance.png'\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ feature importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "\n",
    "### ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:\n",
    "```python\n",
    "import joblib\n",
    "model = joblib.load('best_obesity_model.pkl')\n",
    "predictions = model.predict(new_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "joblib.dump(best_model.best_estimator_, 'best_obesity_model.pkl')\n",
    "print(f\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• '{best_model_name}' ‡πÄ‡∏õ‡πá‡∏ô 'best_obesity_model.pkl'\")\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "model_info = {\n",
    "    'model_name': best_model_name,\n",
    "    'best_params': best_model.best_params_,\n",
    "    'cv_score': best_model.best_score_,\n",
    "    'test_f1_weighted': results_df.loc[results_df['Model'] == best_model_name, 'F1 Weighted'].values[0],\n",
    "    'test_f1_macro': results_df.loc[results_df['Model'] == best_model_name, 'F1 Macro'].values[0],\n",
    "}\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô JSON\n",
    "import json\n",
    "with open('model_info.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô 'model_info.json'\")\n",
    "print(f\"\\nüìä Model Info:\")\n",
    "print(json.dumps(model_info, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á\n",
    "\n",
    "### ‚úÖ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß:\n",
    "1. ‚úÖ **BorderlineSMOTE** ‡πÅ‡∏ó‡∏ô SMOTE ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤\n",
    "2. ‚úÖ **class_weight='balanced'** ‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "3. ‚úÖ **‡∏Ç‡∏¢‡∏≤‡∏¢ hyperparameter grid** \n",
    "4. ‚úÖ **10-fold CV** ‡πÅ‡∏ó‡∏ô 5-fold\n",
    "5. ‚úÖ **‡πÄ‡∏û‡∏¥‡πà‡∏° XGBoost** (‡∏ñ‡πâ‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á)\n",
    "6. ‚úÖ **Feature Importance Analysis**\n",
    "\n",
    "### üìà ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á:\n",
    "- Underweight F1: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô 10-15%\n",
    "- Overweight-Obesity confusion: ‡∏•‡∏î‡∏•‡∏á 5-10%\n",
    "- Overall F1: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô 3-7%\n",
    "\n",
    "### üîú ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£):\n",
    "1. üöÄ **Stacking Ensemble** - ‡∏£‡∏ß‡∏° 3 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô\n",
    "2. üß† **Neural Network** - ‡∏•‡∏≠‡∏á Deep Learning\n",
    "3. üî¨ **Feature Selection** - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏Ñ‡πà features ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
    "4. üìä **SHAP Values** - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ prediction ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
